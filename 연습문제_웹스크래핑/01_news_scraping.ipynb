{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3425920a",
   "metadata": {},
   "source": [
    "웹스크래핑 연습문제1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844a1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.5\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\user\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, charset_normalizer, idna, urllib3\n",
      "Required-by: anaconda-auth, anaconda-catalogs, anaconda-client, anaconda-project, conda, conda-build, conda-libmamba-solver, conda-repo-cli, conda_package_streaming, cookiecutter, datashader, jupyterlab_server, panel, PyGithub, requests-file, requests-toolbelt, Sphinx, streamlit, tldextract\n"
     ]
    }
   ],
   "source": [
    "!pip show requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493e950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/economy\n",
      "<class 'requests.models.Response'>\n",
      "200\n",
      "9\n",
      "<class 'bs4.element.ResultSet'> <class 'bs4.element.Tag'>\n",
      "https://v.daum.net/v/20260206152943998\n",
      "[단독] \"17만 군인 노후자금 녹는다\" 대한토지신탁에 3700억 쏟은 군인공제회 이 기사는 02월 06일 10:03 마켓인사이트에 게재된 기사입니다. 군인공제회의 100% 자회사인 대한토지신탁이 부동산 경기 침체의 직격탄을 맞으며 유동성 위기에 처했다. 군인공제회가 지난 3년간 총 3700억원에 달하는 자금을 지원했음에도 대한토지신탁의 경영 지표는 오히려 악화한 것으로 나타났다. 당장 올 11월 1700억원 규모의 지급보증 만기가 한국경제 3분 전\n",
      "https://v.daum.net/v/20260206151019128\n",
      "동영상     한때 4,900선 밀린 코스피...비트코인도 급락 YTN 22분 전\n",
      "https://v.daum.net/v/20260206150503890\n",
      "정책자금 부당개입 근절 법제화 착수한 중기부…'컨설팅 등록제' 도입 검토 아시아경제 28분 전\n",
      "https://v.daum.net/v/20260206144418089\n",
      "모든 사업장에 퇴직연금 도입...기금형은 DC형에 적용 한국경제TV 48분 전\n",
      "https://v.daum.net/v/20260206143213520\n",
      "[연합뉴스 이 시각 헤드라인] - 14:30 연합뉴스 1시간 전\n",
      "https://v.daum.net/v/20260206141625814\n",
      "퇴직연금 기금으로 굴린다…\"수익률 기대\" 20년 만에 '대수술' 중앙일보 1시간 전\n",
      "https://v.daum.net/v/20260206140600374\n",
      "“AI 시대에도 중심은 사람” 부산시교육청, 대규모 정기인사 한스경제 1시간 전\n",
      "https://v.daum.net/v/20260206140217179\n",
      "노후 걱정? 3억이면 충분하다...통장 4개로 월 200만원 만드는 법은 한국일보 2시간 전\n",
      "https://v.daum.net/v/20260206135537852\n",
      "아내 몰래 사둔 해외 아파트…'난 미국인' 버티더니 결국 [윤지상의 가사언박싱] 한국경제 2시간 전\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 기후 뉴스\n",
    "req_param = {\n",
    "    'sid': 'economy'\n",
    "}\n",
    "\n",
    "# URL 설정\n",
    "url = 'https://news.daum.net/{sid}'.format(**req_param)\n",
    "print(url)\n",
    "\n",
    "# 요청 헤더 설정 : 브라우저 정보\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests 의 get() 함수 호출하기 \n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "# --- 한글 깨짐 방지를 위해 인코딩을 utf-8로 강제 설정 ---\n",
    "res.encoding = 'utf-8' \n",
    "# --------------------------------------------------\n",
    "print(type(res))\n",
    "print(res.status_code)\n",
    "\n",
    "\n",
    "# 응답(response)이 OK 이면\n",
    "if res.ok:\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # 다음 뉴스 목록 영역과 기사 주소 패턴(v.daum.net)으로 변경\n",
    "    print(len(soup.select(\"ul.list_newsheadline2 a[href*='https://v.daum.net/v/']\")))\n",
    "    \n",
    "    # CSS 선택자를 사용해서 a tag 목록 가져오기\n",
    "    a_tags = soup.select(\"ul.list_newsheadline2 a[href*='https://v.daum.net/v/']\")\n",
    "    # ResultSet 타입 및 첫 번째 요소 타입 출력\n",
    "    print(type(a_tags), type(a_tags[0]))\n",
    "    \n",
    "    # <a> 태그 리스트 순회하기 \n",
    "    for a_tag in a_tags:\n",
    "        title = a_tag.text.strip()\n",
    "        link = a_tag['href']\n",
    "        \n",
    "        # 제목과 링크 출력\n",
    "        print(link)\n",
    "        print(title)\n",
    "\n",
    "else:\n",
    "    # 응답(response)이 Error 이면 status code 출력 \n",
    "    print(f'Error Code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ea4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> https://news.daum.net/climate 기후/환경 뉴스 <======\n",
      "https://v.daum.net/v/20260206151627417\n",
      "사육 환경부터 품질까지…강진군, 한우 키우기 '최적지' 전남 강진군은 전라남도 내에서도 청정 자연환경과 풍부한 조사료, 안정적인 사육 인프라를 바탕으로 한우 사육이 활발한 지역이다. 2026년 2월 현재 990농가가 3만 7천두를 사육중이며, 다른 시군에 비해 거세우 비율이 낮고 암소 비율이 72%로 번식우 사육 기반으로 송아지를 생산하는 사육형태를 갖추고 있다. 강진군은 2026년 한우산업 육성을 위해 6 프레시안 16분 전\n",
      "https://v.daum.net/v/20260206142802344\n",
      "노후 석탄발전소 가동하는 美...\"전기요금 폭탄 맞을 것\" 뉴스트리코리아 1시간 전\n",
      "https://v.daum.net/v/20260206141629822\n",
      "영국, 기후재원 지원 약속 1년여 만에 20% 삭감 계획 한겨레 1시간 전\n",
      "https://v.daum.net/v/20260206133151045\n",
      "반도체부터 수소까지…삼척의 7대 산업 로드맵 수소신문 2시간 전\n",
      "https://v.daum.net/v/20260206131717684\n",
      "SMR vs 재생에너지? 이제는 ‘공존 설계’의 시간 수소신문 2시간 전\n",
      "https://v.daum.net/v/20260206123631883\n",
      "동영상     [기후는 말한다] 결국 잘려나간 지느러미…보호구역에서 돌고래 위협? KBS 3시간 전\n",
      "https://v.daum.net/v/20260206123629881\n",
      "동영상     [기후는 말한다] ‘폐그물’ 등 해양폐기물 관리 강화 KBS 3시간 전\n",
      "https://v.daum.net/v/20260206123434849\n",
      "동영상     [기후는 말한다] 월드컵 조별리그 열리는 멕시코에 홍역 확산 KBS 3시간 전\n",
      "https://v.daum.net/v/20260206114430441\n",
      "지구에 단 500마리...멸종한 줄 알았던 숲의 포식자 포착 뉴스펭귄 4시간 전\n"
     ]
    }
   ],
   "source": [
    "section_dict = {\n",
    "    '기후/환경':'climate','사회':'society','경제':'economy','정치':'politics',\n",
    "    '국제':'world','문화':'culture','생활':'life','IT/과학':'tech','인물':'people'\n",
    "}\n",
    "\n",
    "# 함수선언\n",
    "def print_news(section_name):\n",
    "    sid = section_dict.get(section_name)\n",
    "    \n",
    "    if not sid:\n",
    "        print(f\"[{section_name}]은(는) 존재하지 않는 섹션입니다.\")\n",
    "        return\n",
    "\n",
    "    req_param = {\n",
    "        'sid': sid\n",
    "    }\n",
    "\n",
    "    # 3. URL 설정\n",
    "    url = 'https://news.daum.net/{sid}'.format(**req_param)\n",
    "    print(f\"======> {url} {section_name} 뉴스 <======\")\n",
    "\n",
    "    # 4. 요청 헤더 설정 : 브라우저 정보\n",
    "    req_header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # 5. requests 의 get() 함수 호출하기 \n",
    "    res = requests.get(url, headers=req_header)\n",
    "\n",
    "    # --- 한글 깨짐 방지를 위해 인코딩을 utf-8로 강제 설정 ---\n",
    "    res.encoding = 'utf-8' \n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # 6. 응답(response)이 OK 이면\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        # 다음 뉴스 목록 영역과 기사 주소 패턴(v.daum.net)으로 변경하여 a 태그 목록 가져오기\n",
    "        a_tags = soup.select(\"ul.list_newsheadline2 a[href*='https://v.daum.net/v/']\")\n",
    "        \n",
    "        # <a> 태그 리스트 순회하기 \n",
    "        for a_tag in a_tags:\n",
    "            title = a_tag.text.strip()\n",
    "            link = a_tag['href']\n",
    "            \n",
    "            # 제목과 링크 출력 (링크 먼저, 제목 나중)\n",
    "            if title:\n",
    "                print(link)\n",
    "                print(title)\n",
    "\n",
    "    else:\n",
    "        # 응답(response)이 Error 이면 status code 출력 \n",
    "        print(f'Error Code = {res.status_code}')\n",
    "\n",
    "print_news('기후/환경')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c06eae",
   "metadata": {},
   "source": [
    "2-1. Nate 뉴스기사 제목 스크래핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin # 1) urljoin 함수 임포트\n",
    "from IPython.display import Image, display # 2) Image 클래스와 display 함수 임포트\n",
    "\n",
    "# 섹션 설정\n",
    "section_dict = {'최신뉴스':100, '정치':200, '경제':300, '사회':400, '세계':500, 'IT/과학':600 }\n",
    "\n",
    "def print_news(section_name):\n",
    "    sid = section_dict.get(section_name, '200')\n",
    "    url = f'https://news.nate.com/section?mid=n0{sid}'\n",
    "    print(f'{section_name} 뉴스 / {url}')\n",
    "    \n",
    "    req_header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, headers=req_header)\n",
    "    \n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        titles = soup.select(\"h2.tit, strong.tit, span.ltit\")\n",
    "        \n",
    "        for title_tag in titles:\n",
    "            a_tag = title_tag.find_parent('a')\n",
    "            \n",
    "            if a_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                link = a_tag['href']\n",
    "                \n",
    "                # urljoin 함수를 사용하여 기사 링크 경로 합치기\n",
    "                link = urljoin(url, link)\n",
    "                \n",
    "                print(title, link)\n",
    "\n",
    "                # img 엘리먼트의 존재 여부를 체크\n",
    "                item_area = title_tag.find_parent('div') or title_tag.find_parent('li')\n",
    "                img_tag = item_area.select_one('img') if item_area else None\n",
    "\n",
    "                if img_tag and img_tag.get('src'):\n",
    "                    # urljoin 함수를 사용하여 src 속성의 이미지 경로 합치기\n",
    "                    img_src = urljoin(url, img_tag['src'])\n",
    "                    # Image 클래스와 display 함수를 사용하여 이미지 출력\n",
    "                    display(Image(url=img_src))\n",
    "\n",
    "    else:\n",
    "        print(f'Error Code = {res.status_code}')\n",
    "\n",
    "# 호출\n",
    "print_news('사회')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338adc7",
   "metadata": {},
   "source": [
    "2-2. 하나의 네이버 웹툰과 1개의 회차에 대한 Image 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# 함수 선언\n",
    "def download_one_episode(title, no, url):\n",
    "    # 기존 변수명을 유지하기 위해 url을 할당\n",
    "    webtoon_url = url\n",
    "\n",
    "    req_header = {\n",
    "        'referer':webtoon_url,\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "        \n",
    "    }\n",
    "\n",
    "    res = requests.get(webtoon_url, headers=req_header)  \n",
    "    if res.ok:\n",
    "        # .jpg 파일명을 추출해서 list에 저장하기\n",
    "        \n",
    "        soup = BeautifulSoup(res.text,'html.parser')\n",
    "        print(len(soup.select(\"img[src*='IMAG01']\")))\n",
    "        img_tags = soup.select(\"img[src*='IMAG01']\")\n",
    "\n",
    "        #List Comprehension\n",
    "        img_url_list2 = [img_tag['src'] for img_tag in img_tags]\n",
    "        print(img_url_list2[:2])\n",
    "\n",
    "        # 디렉토리 경로 설정 및 생성\n",
    "        # os.makedirs를 사용하여 하위 디렉토리까지 한 번에 생성함\n",
    "        imgdir_name = os.path.join('img', title, str(no))\n",
    "        if not os.path.isdir(imgdir_name):\n",
    "            os.makedirs(imgdir_name)\n",
    "\n",
    "        for img_url in img_url_list2:\n",
    "            # requests 의 get(url, headers) 함수 호출하기 \n",
    "            res = requests.get(img_url, headers=req_header)   \n",
    "            # binary 응답 데이터 가져오기\n",
    "            img_data = res.content\n",
    "            # imgdir_name 아래에 파일 저장\n",
    "            file_path = os.path.join(imgdir_name, os.path.basename(img_url))\n",
    "            # binday data를 file에 write하기\n",
    "            with open(file_path,'wb') as file:\n",
    "                print(f'Writing to {file_path}({len(img_data):,}bytes)')\n",
    "                file.write(img_data)\n",
    "    else:\n",
    "        print(f'Error Code = {res.status_code}')\n",
    "\n",
    "# 함수 호출\n",
    "download_one_episode('일렉시드', 341, 'https://comic.naver.com/webtoon/detail?titleId=717481&no=341&week=wed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd80cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
